#Tensor product hilbert space dimension: 8; Number of simulations: 1;
#Separable states were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2x2/input_data/generated_for_paper/100k/separable.txt; GHZ states were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2x2/input_data/generated_for_paper/100k/GHZ.txt;
#Architecture of the MLP: [512, 256, 128, 32, 1]; Number of epochs: 125; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: adam; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 91.92%
#Sample standard deviation for averaged success rate: 0.0%
#Same average success rate for supplementary tests: [95.1765 99.1295]%
#Sample STD for averaged success rate in supplementary tests: [0. 0.]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.688845	0.000000	0.677987	0.000000
2	0.644711	0.000000	0.603792	0.000000
3	0.532545	0.000000	0.485767	0.000000
4	0.449323	0.000000	0.466966	0.000000
5	0.408626	0.000000	0.419245	0.000000
6	0.387695	0.000000	0.384294	0.000000
7	0.370638	0.000000	0.370211	0.000000
8	0.354604	0.000000	0.353750	0.000000
9	0.343236	0.000000	0.376957	0.000000
10	0.331116	0.000000	0.336633	0.000000
11	0.322729	0.000000	0.334076	0.000000
12	0.310843	0.000000	0.323239	0.000000
13	0.298495	0.000000	0.300462	0.000000
14	0.288447	0.000000	0.308979	0.000000
15	0.279109	0.000000	0.291912	0.000000
16	0.268991	0.000000	0.281442	0.000000
17	0.261867	0.000000	0.285290	0.000000
18	0.255700	0.000000	0.272522	0.000000
19	0.251168	0.000000	0.264989	0.000000
20	0.244903	0.000000	0.265312	0.000000
21	0.241239	0.000000	0.253891	0.000000
22	0.235733	0.000000	0.254707	0.000000
23	0.230916	0.000000	0.251686	0.000000
24	0.226411	0.000000	0.269142	0.000000
25	0.223014	0.000000	0.283400	0.000000
26	0.219018	0.000000	0.245278	0.000000
27	0.217384	0.000000	0.246853	0.000000
28	0.211723	0.000000	0.242515	0.000000
29	0.211118	0.000000	0.240893	0.000000
30	0.206393	0.000000	0.264907	0.000000
31	0.203995	0.000000	0.246665	0.000000
32	0.200612	0.000000	0.251340	0.000000
33	0.198866	0.000000	0.286420	0.000000
34	0.197567	0.000000	0.241996	0.000000
35	0.192159	0.000000	0.241556	0.000000
36	0.190451	0.000000	0.242366	0.000000
37	0.186564	0.000000	0.245455	0.000000
38	0.185578	0.000000	0.252736	0.000000
39	0.182292	0.000000	0.244333	0.000000
40	0.181368	0.000000	0.248625	0.000000
41	0.179192	0.000000	0.231470	0.000000
42	0.176068	0.000000	0.243805	0.000000
43	0.173701	0.000000	0.248146	0.000000
44	0.170885	0.000000	0.248442	0.000000
45	0.170637	0.000000	0.242227	0.000000
46	0.168576	0.000000	0.247288	0.000000
47	0.165891	0.000000	0.249827	0.000000
48	0.163402	0.000000	0.259839	0.000000
49	0.162917	0.000000	0.235458	0.000000
50	0.158771	0.000000	0.253246	0.000000
51	0.157390	0.000000	0.248763	0.000000
52	0.156139	0.000000	0.255614	0.000000
53	0.154780	0.000000	0.255962	0.000000
54	0.152308	0.000000	0.255202	0.000000
55	0.150461	0.000000	0.265248	0.000000
56	0.148774	0.000000	0.261465	0.000000
57	0.147120	0.000000	0.258397	0.000000
58	0.145088	0.000000	0.268006	0.000000
59	0.142640	0.000000	0.266272	0.000000
60	0.142159	0.000000	0.262794	0.000000
61	0.138718	0.000000	0.273680	0.000000
62	0.138355	0.000000	0.275946	0.000000
63	0.135690	0.000000	0.268677	0.000000
64	0.135312	0.000000	0.267442	0.000000
65	0.133396	0.000000	0.276285	0.000000
66	0.131624	0.000000	0.278530	0.000000
