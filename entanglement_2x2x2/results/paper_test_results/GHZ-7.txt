#Tensor product hilbert space dimension: 8; Number of simulations: 2;
#Separable states were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2x2/input_data/generated_for_paper/100k/separable.txt; GHZ states were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2x2/input_data/generated_for_paper/100k/GHZ.txt;
#Architecture of the MLP: [512, 256, 128, 32, 1]; Number of epochs: 125; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: adam; Batch size: 100; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 92.17%
#Sample standard deviation for averaged success rate: 0.4016366517138514%
#Same average success rate for supplementary tests: [95.311  99.2055]%
#Sample STD for averaged success rate in supplementary tests: [0.00919239 0.03959798]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.691283	0.000937	0.680101	0.005223
2	0.651307	0.000685	0.628424	0.001695
3	0.571655	0.000150	0.516129	0.002234
4	0.476633	0.008097	0.456505	0.014215
5	0.425474	0.015768	0.414168	0.015985
6	0.388027	0.013302	0.383513	0.008457
7	0.349514	0.003696	0.356990	0.001156
8	0.320350	0.003403	0.321850	0.008129
9	0.300014	0.007545	0.310253	0.013539
10	0.285417	0.009569	0.309460	0.000533
11	0.272346	0.009411	0.282907	0.012842
12	0.262621	0.008809	0.271900	0.012742
13	0.253334	0.007654	0.273805	0.009965
14	0.247135	0.008046	0.260456	0.011546
15	0.240018	0.006722	0.254577	0.013609
16	0.233810	0.007114	0.259787	0.012662
17	0.229484	0.005620	0.250012	0.011877
18	0.224944	0.004872	0.241879	0.013398
19	0.219577	0.004440	0.245955	0.009921
20	0.216378	0.003832	0.241168	0.005429
21	0.213446	0.002912	0.238670	0.014896
22	0.208506	0.002710	0.247576	0.016179
23	0.206536	0.002743	0.242424	0.007977
24	0.203179	0.001913	0.242940	0.014983
25	0.199586	0.000970	0.233773	0.008534
26	0.195986	0.000862	0.240516	0.003263
27	0.193144	0.000421	0.227824	0.006224
28	0.189895	0.000148	0.232087	0.009478
29	0.187718	0.000764	0.236021	0.010824
30	0.184864	0.000129	0.230720	0.010090
31	0.181036	0.001464	0.235469	0.009764
32	0.179639	0.001301	0.233219	0.012039
33	0.177109	0.001727	0.235381	0.010491
34	0.175375	0.002471	0.238631	0.010597
35	0.172534	0.002109	0.232828	0.011202
36	0.169609	0.002912	0.241623	0.007048
37	0.166752	0.003548	0.237700	0.008969
38	0.164902	0.003076	0.236421	0.011427
39	0.162211	0.003517	0.248308	0.009615
40	0.160603	0.004176	0.237319	0.012170
41	0.157812	0.004722	0.236847	0.016177
42	0.155324	0.003989	0.243417	0.009909
43	0.152586	0.005209	0.249755	0.016736
44	0.150719	0.004871	0.243067	0.018816
45	0.148536	0.006185	0.250538	0.009584
46	0.146139	0.005692	0.243896	0.014167
47	0.143036	0.006128	0.257218	0.017217
48	0.141272	0.006200	0.250802	0.015017
49	0.139027	0.007473	0.265178	0.019623
50	0.136325	0.006768	0.273831	0.024004
51	0.135313	0.007219	0.265705	0.015392
52	0.133013	0.007012	0.255034	0.019720
53	0.141157	0.000000	0.228582	0.000000
54	0.138241	0.000000	0.236479	0.000000
55	0.135525	0.000000	0.241348	0.000000
56	0.135816	0.000000	0.247680	0.000000
57	0.133724	0.000000	0.245230	0.000000
58	0.132400	0.000000	0.248959	0.000000
59	0.131763	0.000000	0.242267	0.000000
60	0.131275	0.000000	0.247855	0.000000
61	0.127796	0.000000	0.251692	0.000000
62	0.124684	0.000000	0.247797	0.000000
63	0.123867	0.000000	0.257621	0.000000
64	0.122743	0.000000	0.270666	0.000000
65	0.119574	0.000000	0.266652	0.000000
66	0.118937	0.000000	0.253265	0.000000
