#Tensor product hilbert space dimension: 8; Number of simulations: 10;
#separable states were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2x2/input_data/100k/separable.txt; GHZ states were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2x2/input_data/100k/GHZ.txt;
#Architecture of the MLP: [256, 128, 64, 32, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: Adam; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 90.17120000000001%
#Sample standard deviation for averaged success rate: 0.09867630825684541%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.690233	0.000464	0.677805	0.001522
2	0.651200	0.001718	0.629405	0.002765
3	0.583058	0.005979	0.545566	0.008199
4	0.508101	0.009226	0.496729	0.010173
5	0.461304	0.011129	0.460387	0.012468
6	0.429228	0.012712	0.434237	0.014693
7	0.404218	0.014096	0.412417	0.017531
8	0.383466	0.015285	0.394706	0.017068
9	0.366855	0.015584	0.380806	0.018364
10	0.353428	0.015406	0.369756	0.017439
11	0.341957	0.015065	0.360049	0.017979
12	0.331633	0.014573	0.349647	0.017615
13	0.321828	0.014207	0.342802	0.016664
14	0.313298	0.013790	0.332433	0.017433
15	0.306473	0.013520	0.327887	0.017996
16	0.299011	0.013087	0.322958	0.016989
17	0.292741	0.012819	0.320758	0.016038
18	0.287041	0.012483	0.315641	0.016511
19	0.281486	0.012269	0.313690	0.017059
20	0.277051	0.011869	0.305319	0.018142
21	0.272352	0.011527	0.302889	0.016115
22	0.266988	0.011238	0.300129	0.017539
23	0.263197	0.011060	0.297100	0.015857
24	0.258953	0.010945	0.296923	0.015227
25	0.255339	0.010690	0.291181	0.015929
26	0.251736	0.010376	0.287438	0.016275
27	0.248418	0.010282	0.284403	0.016812
28	0.245100	0.010158	0.287012	0.015279
29	0.242127	0.009992	0.282743	0.015461
30	0.239125	0.009760	0.279289	0.016515
31	0.235997	0.009664	0.278901	0.014765
32	0.233577	0.009542	0.280475	0.014593
33	0.230721	0.009316	0.275478	0.015098
34	0.228216	0.009235	0.277071	0.016674
35	0.225640	0.009316	0.271881	0.015776
36	0.222806	0.008905	0.271510	0.015184
37	0.220759	0.008866	0.275253	0.016075
38	0.218365	0.008647	0.276383	0.016151
39	0.216168	0.008650	0.274266	0.015548
40	0.213626	0.008508	0.274218	0.016634
41	0.211971	0.008536	0.271988	0.016759
42	0.209575	0.008404	0.270354	0.015927
43	0.207346	0.008359	0.269925	0.015121
44	0.205502	0.008144	0.270553	0.017253
45	0.203632	0.008071	0.269611	0.016285
46	0.202195	0.007938	0.275153	0.016662
47	0.199829	0.007866	0.273226	0.016894
48	0.198074	0.007762	0.274191	0.016083
49	0.196253	0.007764	0.272507	0.016223
50	0.194450	0.007707	0.273784	0.018275
51	0.193098	0.007671	0.271130	0.016814
52	0.191740	0.007571	0.274717	0.017058
53	0.190460	0.008044	0.273680	0.018993
54	0.188945	0.008152	0.270564	0.019572
55	0.186253	0.008995	0.273404	0.022981
56	0.187444	0.009802	0.277770	0.022865
57	0.186625	0.009552	0.275631	0.025367
58	0.184946	0.009598	0.273421	0.026035
59	0.169787	0.000982	0.233571	0.015571
60	0.168501	0.001038	0.241851	0.016846
61	0.167347	0.001068	0.245809	0.016828
62	0.166084	0.000999	0.237997	0.018077
63	0.165143	0.001386	0.221597	0.015826
64	0.164193	0.001383	0.218867	0.014646
65	0.162662	0.001363	0.220336	0.016164
66	0.163388	0.000996	0.207893	0.004049
67	0.163070	0.001527	0.210127	0.004528
68	0.162903	0.001232	0.212608	0.003193
69	0.161375	0.001412	0.208148	0.001787
70	0.160531	0.001568	0.206136	0.000692
71	0.160314	0.001107	0.211087	0.009699
72	0.158767	0.001123	0.205248	0.005417
73	0.158209	0.001077	0.210275	0.006643
74	0.159362	0.000000	0.218878	0.000000
75	0.157359	0.000000	0.211204	0.000000
76	0.156836	0.000000	0.221781	0.000000
77	0.156559	0.000000	0.210475	0.000000
78	0.156764	0.000000	0.212659	0.000000
79	0.154417	0.000000	0.219410	0.000000
80	0.154054	0.000000	0.226102	0.000000
