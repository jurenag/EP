#Tensor product hilbert space dimension: 8; Number of simulations: 10;
#separable states were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2x2/input_data/100k/separable.txt; W states were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2x2/input_data/100k/W.txt;
#Architecture of the MLP: [256, 128, 64, 32, 16, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: Adam; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 98.847%
#Sample standard deviation for averaged success rate: 0.024421544177222067%
#Same average success rate for supplementary tests: [0.8024600000000002, 0.9058634999999999]% [GHZ,BE]
#Sample STD for averaged success rate in supplementary tests: [0.0007833519272970467, 0.0005255400525542765]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.489786	0.005408	0.299084	0.012997
2	0.200434	0.004995	0.155550	0.005647
3	0.126991	0.002719	0.127496	0.004014
4	0.101401	0.002776	0.108316	0.003461
5	0.085794	0.002695	0.091661	0.004140
6	0.074841	0.002704	0.085242	0.003257
7	0.065851	0.002654	0.079035	0.003196
8	0.059232	0.002678	0.076394	0.004665
9	0.053492	0.002569	0.070540	0.003937
10	0.048122	0.002460	0.067314	0.004424
11	0.044113	0.002420	0.065195	0.004020
12	0.040454	0.002372	0.063735	0.002046
13	0.037716	0.002229	0.060003	0.003177
14	0.034616	0.002174	0.060624	0.003646
15	0.032264	0.002080	0.057921	0.004110
16	0.029989	0.002140	0.056435	0.003184
17	0.028442	0.001968	0.052985	0.002738
18	0.026428	0.001888	0.054280	0.003295
19	0.024870	0.001820	0.052525	0.004046
20	0.023412	0.001792	0.052520	0.003445
21	0.022019	0.001721	0.051839	0.002933
22	0.020956	0.001701	0.049117	0.003419
23	0.019868	0.001599	0.052240	0.004446
24	0.019019	0.001551	0.053983	0.003206
25	0.017952	0.001525	0.050321	0.003937
26	0.017162	0.001409	0.050894	0.003941
27	0.016544	0.001403	0.052886	0.004036
28	0.015709	0.001316	0.053131	0.003665
29	0.015134	0.001341	0.051117	0.003299
30	0.014548	0.001209	0.049593	0.003647
31	0.014016	0.001238	0.049198	0.002893
32	0.013521	0.001190	0.048737	0.003350
33	0.012964	0.001051	0.050459	0.004172
34	0.012239	0.001131	0.050607	0.004210
35	0.011615	0.001062	0.046518	0.003703
36	0.011369	0.001091	0.046011	0.003137
37	0.010795	0.000952	0.053685	0.004114
38	0.011168	0.001011	0.048963	0.002551
39	0.010699	0.000953	0.049903	0.003871
40	0.010369	0.000948	0.051476	0.002182
41	0.010386	0.000930	0.047995	0.003403
42	0.010082	0.000881	0.051882	0.005202
43	0.009433	0.000875	0.053199	0.003891
44	0.009352	0.000821	0.047552	0.002923
45	0.009255	0.000829	0.050711	0.004940
46	0.008836	0.000858	0.055229	0.005474
47	0.009224	0.001005	0.055242	0.005184
48	0.009095	0.001018	0.054908	0.005341
49	0.008878	0.000967	0.061664	0.005242
50	0.008546	0.001037	0.051101	0.002492
51	0.007505	0.000679	0.045840	0.003447
52	0.006977	0.000444	0.054924	0.002763
53	0.006780	0.000370	0.048349	0.002132
54	0.006118	0.000302	0.048580	0.001325
55	0.006341	0.000271	0.040838	0.002233
56	0.005995	0.000184	0.043531	0.000672
57	0.005896	0.000130	0.050253	0.003716
58	0.006022	0.000000	0.044273	0.000000
59	0.006057	0.000000	0.043049	0.000000
60	0.006041	0.000000	0.038947	0.000000
