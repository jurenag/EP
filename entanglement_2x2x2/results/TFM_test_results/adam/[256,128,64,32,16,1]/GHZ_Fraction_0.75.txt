#Tensor product hilbert space dimension: 8; Number of simulations: 10;
#separable states were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2x2/input_data/100k/separable.txt; GHZ states were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2x2/input_data/100k/GHZ.txt;
#Architecture of the MLP: [256, 128, 64, 32, 16, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: Adam; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 93.345%
#Sample standard deviation for averaged success rate: 0.09055572593712674%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.692860	0.000185	0.690679	0.001098
2	0.672319	0.002946	0.648933	0.003163
3	0.614300	0.005972	0.571255	0.008389
4	0.529476	0.006941	0.507599	0.007272
5	0.469677	0.008546	0.452114	0.009637
6	0.422648	0.010101	0.413073	0.012074
7	0.382277	0.010793	0.373166	0.012703
8	0.352128	0.010225	0.345403	0.011414
9	0.329512	0.009199	0.330283	0.011219
10	0.312471	0.008307	0.311009	0.008193
11	0.298385	0.007435	0.295164	0.008013
12	0.287568	0.006794	0.287515	0.006957
13	0.278410	0.005996	0.278784	0.004787
14	0.270961	0.004987	0.270509	0.005136
15	0.263631	0.004058	0.265901	0.003834
16	0.257256	0.003390	0.263331	0.002739
17	0.252290	0.002754	0.258345	0.004094
18	0.247573	0.002411	0.251099	0.003625
19	0.242880	0.001943	0.248734	0.002579
20	0.238723	0.001593	0.244762	0.002539
21	0.236096	0.001454	0.241178	0.002265
22	0.232524	0.001252	0.243855	0.004398
23	0.229214	0.001241	0.238563	0.002302
24	0.226269	0.001096	0.233334	0.003110
25	0.224040	0.001053	0.235312	0.002313
26	0.221487	0.001060	0.229185	0.002731
27	0.218545	0.000815	0.225261	0.001615
28	0.215824	0.000931	0.227747	0.003013
29	0.214034	0.000770	0.228122	0.002509
30	0.211909	0.000728	0.221155	0.001986
31	0.209787	0.000907	0.220582	0.002714
32	0.207817	0.000956	0.220159	0.001763
33	0.205706	0.000797	0.217982	0.002186
34	0.203976	0.000660	0.218043	0.003424
35	0.202443	0.000705	0.217715	0.002841
36	0.200177	0.000890	0.216507	0.001651
37	0.198171	0.000734	0.214049	0.001996
38	0.197415	0.000840	0.213668	0.002206
39	0.195427	0.000757	0.212314	0.001749
40	0.193931	0.000770	0.212480	0.002760
41	0.192425	0.000728	0.213372	0.003250
42	0.190881	0.000819	0.213554	0.002826
43	0.189292	0.000708	0.212113	0.002194
44	0.188305	0.000762	0.209187	0.001850
45	0.186857	0.000779	0.210741	0.002471
46	0.185682	0.000733	0.207698	0.002610
47	0.184494	0.000745	0.210814	0.002305
48	0.183036	0.000771	0.207087	0.001307
49	0.181546	0.000841	0.204112	0.001413
50	0.180459	0.000870	0.205301	0.002576
51	0.179247	0.000757	0.207108	0.001800
52	0.178305	0.000744	0.204504	0.002293
53	0.177166	0.000920	0.207562	0.001450
54	0.176160	0.000885	0.205545	0.002077
55	0.175027	0.000939	0.203047	0.002034
56	0.173773	0.000857	0.208613	0.001995
57	0.172732	0.000913	0.202514	0.001943
58	0.171766	0.000900	0.202553	0.002547
59	0.170884	0.001030	0.203505	0.001551
60	0.169892	0.000940	0.204816	0.001779
61	0.168826	0.001032	0.204083	0.002660
62	0.168000	0.000923	0.202688	0.001966
63	0.166777	0.001009	0.203013	0.001919
64	0.165913	0.000921	0.203500	0.002673
65	0.165079	0.000996	0.207514	0.002483
66	0.164561	0.000944	0.205034	0.001486
67	0.164059	0.001090	0.201501	0.002326
68	0.163161	0.001050	0.207287	0.003982
69	0.162639	0.001133	0.210720	0.001641
70	0.161873	0.001236	0.204810	0.003405
71	0.160163	0.001450	0.208160	0.003760
72	0.159733	0.001268	0.203565	0.003875
73	0.158922	0.001685	0.201558	0.003233
74	0.158216	0.001696	0.198087	0.003727
75	0.157200	0.001594	0.208207	0.003801
76	0.156854	0.001591	0.204997	0.003959
77	0.155788	0.001768	0.201375	0.003007
78	0.154776	0.001677	0.202038	0.003342
79	0.156754	0.002110	0.201074	0.003654
80	0.156047	0.002621	0.198227	0.003141
81	0.154375	0.002136	0.203235	0.004303
82	0.154522	0.002337	0.199586	0.003754
83	0.153393	0.002419	0.199372	0.001176
84	0.154443	0.002588	0.191237	0.003732
85	0.154086	0.002459	0.193410	0.001419
86	0.152352	0.002575	0.200194	0.005362
87	0.152173	0.002417	0.198116	0.002555
88	0.155794	0.000000	0.192910	0.000000
89	0.155663	0.000000	0.194468	0.000000
90	0.154174	0.000000	0.199946	0.000000
91	0.154141	0.000000	0.178945	0.000000
92	0.153264	0.000000	0.189153	0.000000
93	0.153139	0.000000	0.196589	0.000000
94	0.150803	0.000000	0.190121	0.000000
95	0.151492	0.000000	0.191910	0.000000
96	0.151221	0.000000	0.187435	0.000000
97	0.150303	0.000000	0.189227	0.000000
98	0.149712	0.000000	0.199645	0.000000
99	0.149300	0.000000	0.197417	0.000000
100	0.148114	0.000000	0.211593	0.000000
