#Tensor product hilbert space dimension: 8; Number of simulations: 5;
#separable states were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2x2/input_data/100k/separable.txt; GHZ states were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2x2/input_data/100k/GHZ.txt;
#Architecture of the MLP: [128, 64, 32, 16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: Adam; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 89.1664%
#Sample standard deviation for averaged success rate: 0.10237901259926305%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.693097	0.000040	0.692379	0.000283
2	0.685116	0.000888	0.672498	0.001393
3	0.652559	0.002105	0.635045	0.003372
4	0.608168	0.007189	0.588305	0.013881
5	0.559829	0.015577	0.547659	0.015742
6	0.523302	0.013230	0.514231	0.012539
7	0.492022	0.011574	0.489964	0.014756
8	0.467770	0.012886	0.475719	0.014197
9	0.450062	0.014161	0.456372	0.016373
10	0.436044	0.015378	0.441541	0.020295
11	0.424088	0.016142	0.430364	0.018555
12	0.413725	0.016885	0.423240	0.023234
13	0.403756	0.017919	0.424410	0.018218
14	0.395027	0.018737	0.407290	0.024084
15	0.386595	0.019312	0.402271	0.025187
16	0.379172	0.020145	0.399111	0.024638
17	0.372101	0.019946	0.382984	0.024211
18	0.365703	0.020049	0.383082	0.020113
19	0.359364	0.020003	0.382807	0.021713
20	0.354033	0.019462	0.366644	0.024404
21	0.348607	0.019608	0.370736	0.020541
22	0.343547	0.019365	0.363513	0.022524
23	0.338792	0.019350	0.357449	0.021941
24	0.335008	0.018804	0.350731	0.022412
25	0.330700	0.018869	0.349490	0.022862
26	0.326667	0.019046	0.355258	0.021075
27	0.323536	0.018493	0.341648	0.023714
28	0.319810	0.018473	0.344967	0.020919
29	0.317094	0.018455	0.334845	0.022556
30	0.313973	0.018271	0.338912	0.025015
31	0.311103	0.018284	0.338743	0.022098
32	0.308190	0.017975	0.334760	0.024910
33	0.305272	0.017944	0.327983	0.022270
34	0.303199	0.017753	0.332726	0.020813
35	0.300935	0.017487	0.325780	0.023846
36	0.297925	0.017691	0.326534	0.022276
37	0.295889	0.017577	0.324791	0.022216
38	0.293827	0.017769	0.321700	0.021685
39	0.291876	0.017420	0.315189	0.023753
40	0.290109	0.016951	0.315775	0.021235
41	0.287677	0.017367	0.318008	0.021527
42	0.285650	0.017079	0.316747	0.021276
43	0.284132	0.017078	0.314590	0.022444
44	0.282182	0.017100	0.312514	0.020780
45	0.280505	0.016841	0.310181	0.021366
46	0.279023	0.016908	0.309272	0.023861
47	0.276973	0.016496	0.311950	0.020769
48	0.275640	0.016584	0.310667	0.020576
49	0.274324	0.016624	0.310551	0.020874
50	0.272610	0.016535	0.303411	0.021594
51	0.271849	0.016500	0.304446	0.023800
52	0.269545	0.016574	0.302636	0.020630
53	0.268369	0.016207	0.304694	0.020823
54	0.266781	0.016313	0.311551	0.020986
55	0.265562	0.015976	0.297274	0.023778
56	0.264088	0.016198	0.300029	0.023480
57	0.263326	0.016280	0.301066	0.020687
58	0.261786	0.016164	0.299751	0.022565
59	0.260389	0.016266	0.297361	0.021579
60	0.258687	0.016358	0.298150	0.020286
61	0.258058	0.016243	0.295430	0.021487
62	0.257107	0.016273	0.291376	0.020251
63	0.256131	0.016277	0.297727	0.024877
64	0.254338	0.016143	0.289835	0.023219
65	0.253441	0.016017	0.295169	0.023915
66	0.252708	0.016130	0.293761	0.026324
67	0.251455	0.016120	0.291552	0.022399
68	0.249922	0.016231	0.290338	0.022654
69	0.249645	0.015956	0.291731	0.022868
70	0.248369	0.016081	0.298151	0.028402
71	0.247315	0.016219	0.290042	0.022424
72	0.246570	0.015890	0.290902	0.021143
73	0.245946	0.016328	0.289411	0.023057
74	0.244953	0.016129	0.288837	0.023443
75	0.230074	0.012767	0.269560	0.020813
76	0.229500	0.012515	0.276976	0.016954
77	0.228647	0.012694	0.267664	0.017260
78	0.227886	0.012392	0.274197	0.021658
79	0.226293	0.012829	0.262522	0.019920
80	0.225618	0.012521	0.269643	0.014137
81	0.224805	0.012112	0.266381	0.017144
82	0.224110	0.011994	0.276820	0.012223
83	0.223288	0.012001	0.264897	0.018419
84	0.222575	0.011903	0.264023	0.018576
85	0.221447	0.012074	0.262697	0.017478
86	0.221589	0.011839	0.260270	0.017780
87	0.220439	0.011835	0.263614	0.019303
88	0.219852	0.011623	0.273308	0.015927
89	0.218861	0.011836	0.265162	0.017639
90	0.218763	0.011751	0.261400	0.020209
91	0.217047	0.011786	0.263705	0.019588
92	0.216819	0.011708	0.267234	0.020021
93	0.216184	0.011796	0.268248	0.018031
94	0.215251	0.011602	0.264005	0.018836
95	0.214855	0.011479	0.260384	0.015995
96	0.214465	0.011595	0.260590	0.018331
97	0.213638	0.011318	0.261260	0.019661
98	0.213063	0.011301	0.258905	0.018153
99	0.212722	0.011518	0.261306	0.017563
100	0.211975	0.011470	0.260856	0.018562
