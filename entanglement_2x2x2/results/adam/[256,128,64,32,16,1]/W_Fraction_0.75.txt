#Tensor product hilbert space dimension: 8; Number of simulations: 10;
#separable states were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2x2/input_data/100k/separable.txt; W states were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2x2/input_data/100k/W.txt;
#Architecture of the MLP: [256, 128, 64, 32, 16, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: Adam; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 98.79140000000001%
#Sample standard deviation for averaged success rate: 0.025764039295110383%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.488177	0.006293	0.290640	0.012650
2	0.195629	0.007264	0.149783	0.004276
3	0.125181	0.003268	0.121778	0.004430
4	0.099757	0.002908	0.100334	0.003500
5	0.083617	0.002901	0.093625	0.004587
6	0.072327	0.002803	0.082143	0.003268
7	0.063855	0.002666	0.076045	0.003384
8	0.057075	0.002673	0.077500	0.003770
9	0.051431	0.002469	0.070889	0.003478
10	0.046594	0.002353	0.066094	0.003504
11	0.042877	0.002259	0.064500	0.003365
12	0.039356	0.002105	0.060919	0.003642
13	0.036264	0.002097	0.060993	0.003999
14	0.033763	0.001978	0.057138	0.003179
15	0.031225	0.001804	0.056069	0.003181
16	0.029262	0.001843	0.058934	0.004074
17	0.027373	0.001710	0.055031	0.003047
18	0.025804	0.001647	0.052920	0.003531
19	0.024137	0.001611	0.051159	0.002482
20	0.022880	0.001491	0.054503	0.003084
21	0.021512	0.001496	0.054441	0.003948
22	0.020450	0.001439	0.050014	0.003598
23	0.019467	0.001369	0.053758	0.003170
24	0.018392	0.001292	0.052671	0.003781
25	0.017516	0.001318	0.056171	0.004028
26	0.016902	0.001182	0.050069	0.002991
27	0.016212	0.001173	0.051040	0.002674
28	0.015207	0.001083	0.050402	0.003309
29	0.014773	0.001107	0.052220	0.003042
30	0.014330	0.001020	0.048664	0.003621
31	0.013502	0.001029	0.051261	0.003744
32	0.013152	0.001011	0.050439	0.003555
33	0.012722	0.000975	0.050219	0.002258
34	0.012262	0.000856	0.050783	0.003653
35	0.011259	0.000886	0.050373	0.004089
36	0.011098	0.000898	0.051376	0.005008
37	0.010798	0.000916	0.050932	0.004933
38	0.010464	0.001061	0.053357	0.005131
39	0.010380	0.001046	0.053486	0.005737
40	0.009946	0.001082	0.048176	0.004878
41	0.009817	0.000961	0.047700	0.003738
42	0.009045	0.000897	0.049637	0.003261
43	0.008171	0.000998	0.042201	0.004730
44	0.007905	0.000962	0.045572	0.006890
45	0.006811	0.000779	0.047342	0.006041
46	0.008356	0.000872	0.049350	0.008487
47	0.007619	0.001168	0.048150	0.004559
48	0.007917	0.001003	0.049942	0.004829
49	0.005583	0.000000	0.045491	0.000000
50	0.006250	0.000000	0.034155	0.000000
51	0.006028	0.000000	0.041850	0.000000
52	0.005827	0.000000	0.032367	0.000000
53	0.005669	0.000000	0.043450	0.000000
54	0.005492	0.000000	0.039236	0.000000
55	0.005353	0.000000	0.035694	0.000000
56	0.005316	0.000000	0.033728	0.000000
57	0.005305	0.000000	0.042885	0.000000
58	0.004668	0.000000	0.035639	0.000000
