#Tensor product hilbert space dimension: 8; Number of simulations: 10;
#separable states were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2x2/input_data/100k/separable.txt; W states were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2x2/input_data/100k/W.txt;
#Architecture of the MLP: [256, 128, 64, 32, 16, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 98.73420000000002%
#Sample standard deviation for averaged success rate: 0.023604978440996436%
#Same average success rate for supplementary tests: [0.8188489999999996, 0.9167215000000002]%
#Sample STD for averaged success rate in supplementary tests: [0.0007434275862483204, 0.00047880654463859315]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.483918	0.005896	0.295478	0.012508
2	0.209679	0.008308	0.160500	0.005312
3	0.130182	0.004875	0.126240	0.008856
4	0.100351	0.004422	0.105429	0.003769
5	0.086438	0.004101	0.098303	0.006114
6	0.079420	0.004219	0.097245	0.006581
7	0.075439	0.004299	0.083013	0.005672
8	0.071807	0.004370	0.086229	0.005289
9	0.070102	0.004484	0.083924	0.005663
10	0.067804	0.004657	0.083889	0.006121
11	0.066981	0.004606	0.085477	0.006009
12	0.064805	0.004416	0.077320	0.006492
13	0.063240	0.004295	0.076249	0.004472
14	0.063084	0.004328	0.081859	0.008652
15	0.061084	0.004315	0.076064	0.004839
16	0.059690	0.004032	0.085995	0.012802
17	0.058838	0.003893	0.070435	0.004879
18	0.058818	0.004358	0.078600	0.010182
19	0.058214	0.004913	0.081865	0.012431
20	0.056837	0.005273	0.073402	0.005590
21	0.055835	0.005180	0.082726	0.013942
22	0.056037	0.005167	0.070924	0.009870
23	0.054910	0.005046	0.070377	0.005127
24	0.052878	0.005000	0.070853	0.007291
25	0.050943	0.004976	0.070232	0.006660
26	0.051092	0.005145	0.171946	0.103817
27	0.050587	0.005401	0.069784	0.013679
28	0.050964	0.005378	0.065002	0.007732
29	0.050030	0.005388	0.095374	0.020072
30	0.049012	0.005710	0.071531	0.011144
31	0.049815	0.005805	0.061128	0.005222
32	0.047869	0.005778	0.065475	0.006066
33	0.041510	0.003946	0.063577	0.005098
34	0.040461	0.003758	0.054607	0.004185
35	0.040346	0.004279	0.052228	0.003342
36	0.039481	0.003940	0.053511	0.005602
37	0.037761	0.003227	0.099960	0.037951
38	0.037560	0.003030	0.052928	0.004370
39	0.039120	0.003632	0.147988	0.068420
40	0.037903	0.003314	0.052040	0.002886
41	0.038846	0.003814	0.070929	0.009907
42	0.038529	0.002819	0.057616	0.003298
43	0.038737	0.003128	0.060542	0.004346
44	0.038333	0.002809	0.055706	0.003615
45	0.037787	0.002582	0.065181	0.005985
46	0.036349	0.002182	0.058628	0.003243
47	0.036903	0.003017	0.055310	0.004021
48	0.041328	0.006857	0.055911	0.003294
49	0.041768	0.006590	0.053933	0.002739
50	0.036192	0.002107	0.056162	0.005420
51	0.034873	0.002395	0.064030	0.008427
52	0.036677	0.003678	0.053919	0.004182
53	0.037542	0.003784	0.067392	0.011271
54	0.031591	0.002868	0.069652	0.008073
55	0.030900	0.002344	0.043707	0.006033
56	0.031182	0.002826	0.047582	0.007797
57	0.032406	0.003294	0.049549	0.007743
58	0.032921	0.004689	0.072378	0.010699
59	0.032010	0.004185	0.085693	0.020355
60	0.030200	0.003750	0.044110	0.008057
61	0.031754	0.003204	0.053550	0.006949
62	0.031778	0.002646	0.063099	0.006782
63	0.030627	0.005012	0.048328	0.004879
64	0.030991	0.003052	0.050951	0.003729
65	0.030112	0.004570	0.057608	0.010000
66	0.029030	0.004083	0.053158	0.007163
67	0.029726	0.003871	0.045264	0.003755
68	0.028377	0.003767	0.052251	0.009548
69	0.029273	0.003608	0.052571	0.006618
70	0.029302	0.003666	0.063468	0.009501
71	0.032847	0.004624	0.056179	0.012331
72	0.029005	0.003678	0.055940	0.003869
73	0.029900	0.003813	0.051425	0.008012
74	0.028982	0.004616	0.044858	0.005107
75	0.028555	0.003366	0.055335	0.002540
76	0.032540	0.000000	0.050475	0.000000
77	0.034479	0.000000	0.070241	0.000000
78	0.035791	0.000000	0.059081	0.000000
79	0.034419	0.000000	0.084693	0.000000
80	0.033732	0.000000	0.056549	0.000000
81	0.031701	0.000000	0.096205	0.000000
82	0.033762	0.000000	0.067276	0.000000
