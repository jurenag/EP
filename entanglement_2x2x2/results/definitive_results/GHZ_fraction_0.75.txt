#Tensor product hilbert space dimension: 8; Number of simulations: 10;
#separable states were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2x2/input_data/100k/separable.txt; GHZ states were read from: /home/julio/Documents/my_projects/TFM/code/entanglement_2x2x2/input_data/100k/GHZ.txt;
#Architecture of the MLP: [256, 128, 64, 32, 16, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: Adam; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 93.14260000000002%
#Sample standard deviation for averaged success rate: 0.09194238035204434%
#Same average success rate for supplementary tests: [0.9922160000000001, 0.9550404999999998]% [w, BE]
#Sample STD for averaged success rate in supplementary tests: [7.24399525123989e-05, 0.0003445170050937272]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.692675	0.000206	0.689142	0.001443
2	0.669944	0.003595	0.648257	0.005224
3	0.612709	0.010094	0.576605	0.014004
4	0.528386	0.015793	0.496266	0.016858
5	0.468009	0.017853	0.449080	0.017646
6	0.421282	0.016173	0.410048	0.015355
7	0.385038	0.013248	0.378928	0.012956
8	0.356620	0.011219	0.346658	0.010343
9	0.335087	0.009643	0.330657	0.009845
10	0.318451	0.008630	0.324184	0.010110
11	0.305884	0.007746	0.303506	0.008258
12	0.294963	0.006945	0.297269	0.008348
13	0.285891	0.006499	0.290137	0.007350
14	0.278305	0.006096	0.277436	0.006111
15	0.271750	0.005736	0.272665	0.006712
16	0.265886	0.005366	0.269618	0.005798
17	0.260293	0.005085	0.262480	0.006537
18	0.255109	0.004949	0.256465	0.004955
19	0.250775	0.004720	0.259335	0.005290
20	0.246533	0.004366	0.250080	0.005551
21	0.242651	0.004386	0.248415	0.005705
22	0.238886	0.004034	0.246128	0.005770
23	0.235788	0.003937	0.242547	0.004614
24	0.232479	0.004013	0.241488	0.004217
25	0.229606	0.003804	0.237350	0.005144
26	0.227106	0.003717	0.240668	0.005457
27	0.224065	0.003697	0.232382	0.004599
28	0.222037	0.003752	0.231033	0.004616
29	0.219395	0.003576	0.235489	0.005204
30	0.217790	0.003572	0.231234	0.004438
31	0.215338	0.003561	0.240254	0.006517
32	0.213519	0.003591	0.225670	0.005212
33	0.211153	0.003618	0.225932	0.004100
34	0.209499	0.003420	0.222831	0.005034
35	0.207582	0.003490	0.226563	0.005087
36	0.205866	0.003251	0.227018	0.003987
37	0.204269	0.003297	0.221108	0.004902
38	0.202326	0.003268	0.219149	0.004453
39	0.200720	0.003270	0.219172	0.004972
40	0.199053	0.003226	0.215467	0.004163
41	0.197978	0.003079	0.214522	0.004282
42	0.196317	0.003047	0.217604	0.005208
43	0.194641	0.003068	0.212145	0.005045
44	0.193114	0.002877	0.217716	0.005861
45	0.191908	0.002954	0.211858	0.005327
46	0.190507	0.002891	0.214700	0.005472
47	0.189310	0.002783	0.213300	0.003298
48	0.187958	0.002732	0.212127	0.005083
49	0.186901	0.002660	0.211937	0.005102
50	0.185134	0.002725	0.209364	0.004237
51	0.184361	0.002578	0.209400	0.003892
52	0.183249	0.002727	0.207708	0.004473
53	0.181927	0.002609	0.206472	0.004825
54	0.180663	0.002489	0.211847	0.004672
55	0.180402	0.002654	0.207214	0.005867
56	0.179453	0.002610	0.206518	0.004821
57	0.178389	0.002510	0.207882	0.004966
58	0.177372	0.002513	0.211039	0.004838
59	0.176512	0.002801	0.209319	0.004294
60	0.175519	0.002808	0.214505	0.005942
61	0.174795	0.002784	0.204308	0.005669
62	0.173821	0.002673	0.207335	0.005475
63	0.170047	0.001711	0.199254	0.002244
64	0.168946	0.001666	0.199168	0.002695
65	0.169497	0.001709	0.207759	0.001605
66	0.168197	0.002055	0.203454	0.002742
67	0.167636	0.002316	0.199773	0.001497
68	0.165957	0.001916	0.207053	0.004653
69	0.165784	0.001975	0.199580	0.003145
70	0.164556	0.002303	0.204072	0.001845
71	0.163652	0.001877	0.204585	0.004856
72	0.164548	0.002018	0.206239	0.002971
73	0.164818	0.002575	0.202177	0.002892
74	0.164102	0.002819	0.199210	0.004558
75	0.163573	0.002850	0.201176	0.002785
76	0.164019	0.003525	0.209626	0.005302
77	0.163318	0.003665	0.204185	0.007650
78	0.168335	0.000000	0.205551	0.000000
79	0.166796	0.000000	0.198763	0.000000
80	0.166091	0.000000	0.209066	0.000000
81	0.165501	0.000000	0.207313	0.000000
82	0.165815	0.000000	0.213671	0.000000
83	0.164055	0.000000	0.207011	0.000000
84	0.163245	0.000000	0.208472	0.000000
85	0.163880	0.000000	0.208074	0.000000
86	0.162627	0.000000	0.201364	0.000000
