#Tensor product hilbert space dimension: 8; Number of simulations: 10;
#separable states were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2x2/input_data/100k/separable.txt; Bipartitely entangled states were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2x2/input_data/100k/bipartite_entanglement.txt;
#Architecture of the MLP: [256, 128, 64, 32, 16, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 95.23119999999999%
#Sample standard deviation for averaged success rate: 0.06776113141912549%
#Same average success rate for supplementary tests: [0.8610720000000003, 0.981298]%
#Sample STD for averaged success rate in supplementary tests: [0.0006286108128882276, 0.00011173037178851643]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.661945	0.002505	0.602869	0.013648
2	0.498252	0.019256	0.408873	0.021019
3	0.358008	0.016222	0.313770	0.016156
4	0.290355	0.013365	0.268947	0.011690
5	0.250265	0.011808	0.238845	0.012655
6	0.226693	0.011473	0.241248	0.020262
7	0.209083	0.010640	0.208707	0.010766
8	0.194596	0.009504	0.206218	0.013834
9	0.184100	0.008662	0.221720	0.031809
10	0.176335	0.008659	0.214899	0.024910
11	0.170695	0.008349	0.194675	0.010532
12	0.166000	0.008241	0.185247	0.009624
13	0.161754	0.008284	0.179831	0.009850
14	0.157230	0.008205	0.174548	0.009814
15	0.154961	0.008119	0.172518	0.008612
16	0.152182	0.007817	0.181962	0.016996
17	0.149427	0.008153	0.178105	0.009688
18	0.146887	0.007939	0.168458	0.006813
19	0.145082	0.007753	0.178309	0.013116
20	0.143381	0.006912	0.158498	0.006920
21	0.141146	0.006156	0.178518	0.012034
22	0.139613	0.005060	0.158799	0.004658
23	0.139179	0.004804	0.172013	0.005169
24	0.138770	0.004730	0.162957	0.003480
25	0.136575	0.004325	0.160942	0.004757
26	0.137519	0.004138	0.163736	0.005172
27	0.135909	0.003658	0.173595	0.016192
28	0.133195	0.003561	0.155010	0.003718
29	0.133802	0.003408	0.157561	0.004370
30	0.132072	0.003285	0.171782	0.006050
31	0.136259	0.004718	0.223213	0.045629
32	0.139446	0.004719	0.163328	0.006042
33	0.138189	0.004119	0.199249	0.018226
34	0.133811	0.004301	0.175869	0.014033
35	0.139757	0.005508	0.173538	0.009980
36	0.141747	0.010472	0.170673	0.011471
37	0.135198	0.005854	0.191794	0.014969
38	0.135437	0.008278	0.159378	0.004913
39	0.130768	0.007606	0.166573	0.013283
40	0.134387	0.009027	0.188413	0.014565
41	0.130736	0.010535	0.179666	0.021382
42	0.129257	0.009023	0.161048	0.009301
43	0.138756	0.012569	0.202332	0.019150
44	0.145740	0.014065	0.154599	0.008406
45	0.148713	0.018031	0.163692	0.009721
46	0.138079	0.013180	0.198763	0.009703
47	0.146771	0.016808	0.170264	0.013212
48	0.141185	0.011750	0.194547	0.014064
49	0.139794	0.012964	0.180588	0.001378
50	0.123141	0.000000	0.187476	0.000000
51	0.123063	0.000000	0.170139	0.000000
52	0.131729	0.000000	0.219199	0.000000
53	0.120580	0.000000	0.163340	0.000000
54	0.119353	0.000000	0.328627	0.000000
55	0.122942	0.000000	0.189955	0.000000
