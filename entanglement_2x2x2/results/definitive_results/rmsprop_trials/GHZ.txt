#Tensor product hilbert space dimension: 8; Number of simulations: 10;
#separable states were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2x2/input_data/100k/separable.txt; GHZ states were read from: /home/julio/Documents/jupyterenvironment/TFM/code/entanglement_2x2x2/input_data/100k/GHZ.txt;
#Architecture of the MLP: [256, 128, 64, 32, 16, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 83.89460000000001%
#Sample standard deviation for averaged success rate: 0.11215614925986005%
#Same average success rate for supplementary tests: [0.9596065000000001, 0.905427]%
#Sample STD for averaged success rate in supplementary tests: [0.00015969212813645166, 0.0003573307128627474]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.691330	0.000447	0.681946	0.001928
2	0.662881	0.001651	0.644608	0.002556
3	0.619035	0.002810	0.593875	0.003995
4	0.567809	0.003435	0.552095	0.004952
5	0.528503	0.005009	0.522085	0.010136
6	0.500907	0.007686	0.497558	0.010029
7	0.479869	0.009845	0.479427	0.013331
8	0.463772	0.011726	0.479838	0.019978
9	0.452065	0.013311	0.459701	0.015206
10	0.442656	0.014356	0.465633	0.016725
11	0.435500	0.015208	0.444303	0.016257
12	0.429323	0.015379	0.450289	0.017961
13	0.423035	0.016006	0.449457	0.021876
14	0.418650	0.016407	0.437766	0.019783
15	0.413667	0.016372	0.435647	0.018758
16	0.408472	0.016093	0.436482	0.020660
17	0.403568	0.016643	0.434185	0.018957
18	0.399651	0.016750	0.445953	0.033927
19	0.396367	0.017107	0.434792	0.029053
20	0.390535	0.016543	0.446598	0.040637
21	0.386419	0.017033	0.462846	0.055927
22	0.381440	0.017559	0.407542	0.020449
23	0.378408	0.017602	0.416076	0.027780
24	0.374041	0.017655	0.400754	0.021771
25	0.358822	0.015546	0.390168	0.017842
26	0.357088	0.015802	0.381317	0.017048
27	0.353348	0.015626	0.380064	0.018991
28	0.352945	0.015288	0.401828	0.027701
29	0.350036	0.015278	0.400326	0.023405
30	0.349488	0.015406	0.394759	0.018469
31	0.347586	0.015387	0.386625	0.018531
32	0.345041	0.015505	0.388759	0.023104
33	0.345076	0.016119	0.394486	0.020326
34	0.346501	0.015490	0.376911	0.020123
35	0.346105	0.016530	0.378397	0.024422
36	0.342204	0.016153	0.393518	0.022349
37	0.339324	0.015478	0.391231	0.024491
38	0.330743	0.015403	0.385251	0.022165
39	0.326504	0.014131	0.382335	0.025611
40	0.329278	0.014992	0.369131	0.019668
41	0.325269	0.014075	0.396270	0.033865
42	0.319538	0.013101	0.372769	0.025425
43	0.316644	0.012335	0.374036	0.020929
44	0.315087	0.011463	0.363595	0.022776
45	0.317954	0.011344	0.361087	0.008303
46	0.315353	0.010662	0.393062	0.032524
47	0.313984	0.011341	0.373322	0.018265
48	0.313743	0.011752	0.344185	0.015319
49	0.315697	0.011468	0.385964	0.030847
50	0.315043	0.010545	0.543297	0.116315
51	0.311107	0.011734	0.388054	0.041139
52	0.316538	0.012940	0.373202	0.027518
53	0.325530	0.003934	0.386739	0.018009
54	0.327382	0.003688	0.457762	0.086687
55	0.331338	0.008862	0.380056	0.009533
56	0.319319	0.004011	0.424719	0.040894
57	0.328165	0.006201	0.377755	0.021316
58	0.317455	0.004999	0.370799	0.024214
59	0.315936	0.004660	0.372186	0.020646
60	0.317516	0.005782	0.330625	0.008602
61	0.315287	0.005795	0.411348	0.016241
62	0.324035	0.004935	0.354864	0.024583
63	0.314269	0.007263	0.351924	0.005421
64	0.315001	0.008186	0.344080	0.020540
65	0.311117	0.008949	0.369395	0.018624
66	0.314806	0.008795	0.455157	0.030915
67	0.314887	0.007366	0.365430	0.037191
68	0.311833	0.009232	0.421318	0.004637
69	0.312210	0.010355	0.358459	0.006984
70	0.310021	0.013379	0.363146	0.008121
71	0.311496	0.013588	0.433474	0.041769
72	0.316661	0.004598	0.375279	0.002755
73	0.309366	0.010584	0.388788	0.014551
74	0.312504	0.012901	0.383442	0.020155
75	0.316293	0.009347	0.372309	0.029247
76	0.329922	0.000000	0.443078	0.000000
77	0.331000	0.000000	0.355679	0.000000
78	0.339465	0.000000	0.497541	0.000000
79	0.345626	0.000000	0.363412	0.000000
80	0.335465	0.000000	0.547515	0.000000
