#Tensor product hilbert space dimension: 8; Number of simulations: 10;
#Separable states were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2x2/input_data/generated_for_paper/100k/separable.txt; W states were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2x2/input_data/generated_for_paper/100k/W.txt;
#Architecture of the MLP: [512, 256, 128, 32, 1]; Number of epochs: 125; Fraction of DMs used for training: 0.75;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: adam; Batch size: 40; Test tolerance: 0.5;
#Sucess rate averaged over every simulation and over every sample in the test set: 98.65820000000001%
#Sample standard deviation for averaged success rate: 0.11495753998739129%
#Same average success rate for supplementary tests: [88.3133 78.2762]%
#Sample STD for averaged success rate in supplementary tests: [0.50528996 0.42750495]%
#
#       Further info on simulation parameters
# 
#       N=8;                                 
#       howManyTimes = 10;                   
#       architecture = [512,256,128,32,1];   
#       max_nepochs = 125;                   
#       fraction=0.75;                       
#       actHL = 'relu';                      
#       lastAct = 'sigmoid';                 
#       loss_f = 'binary_crossentropy'       
#       batch_size = 40;                     
#       output_file = cwd + '/results/paper_definitive_results/TW_W.txt'
#       opt = 'adam'                          
#       take_supplementary_tests = True       
#       tol = 0.5                             
#       study_val_loss = True                 
#       early_stopping = True                 
#       skipped_rows = (0,0,0,0,0,0)          
#       pre_shuffle = True                    
#       first_type = 'Separable'              
#       second_type= 'W'                      
#       metric = 'val_loss'                   
#       epochs_to_wait = 25                   
#       min_delta = 1e-3                      
#
#       
#       loss_evolution, loss_evolution_std, ASR,        \
#       ASRSTD, ASR2, ASRSTD2, val_loss, val_loss_std,  \
#       reached_this_epoch, longest_training =          \
#       binaryOutput_formatData_trainNN_averageLoss_averageTestResults_and_writeResults(N, howManyTimes, first_filepath, 
#                                                                                       first_type, second_filepath, 
#                                                                                       second_type, architecture, 
#                                                                                       max_nepochs, fraction, 
#                                                                                       actHL, lastAct, loss_f, 
#                                                                                       batch_size, optimizer=opt, 
#                                                                                       perform_additional_tests=take_supplementary_tests, 
#                                                                                       first_test_filepath=additional_separable_filepaths, 
#                                                                                       second_test_filepath=additional_entangled_filepaths, 
#                                                                                       outFilePath=output_file, tolerance=tol, 
#                                                                                       use_validation_data=study_val_loss, 
#                                                                                       trigger_early_stopping=early_stopping, 
#                                                                                       metric_to_monitor=metric, 
#                                                                                       epochs_patience=epochs_to_wait, 
#                                                                                       min_improvement=min_delta, monitor_mode='min', 
#                                                                                       baseline=None, 
#                                                                                       recover_best_configuration=True, 
#                                                                                       first_label=0.0, second_label=1.0, 
#                                                                                       shuffle=pre_shuffle, rts=skipped_rows, 
#                                                                                       verb=0, snitch_every=1)
#
#       reached_this_epoch, longest_training = [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10
#                                               10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10  9  8  8  8  8  7  5  5
#                                               5  5  5  5  5  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4
#                                               4  4  3  2  1  1  1  1  1  1  1  1  1  1], 86
#
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.442119	0.003453	0.223886	0.006910
2	0.155746	0.002078	0.134190	0.003560
3	0.107295	0.001621	0.106472	0.001511
4	0.087958	0.001619	0.097665	0.003755
5	0.075693	0.001749	0.093494	0.001953
6	0.066805	0.001745	0.083483	0.002358
7	0.058875	0.001653	0.078300	0.002146
8	0.052875	0.001714	0.075846	0.003118
9	0.047690	0.001678	0.074243	0.003388
10	0.042940	0.001575	0.074393	0.003052
11	0.039271	0.001569	0.073279	0.003876
12	0.035857	0.001373	0.069655	0.004214
13	0.032819	0.001399	0.069182	0.004445
14	0.030536	0.001353	0.064422	0.003192
15	0.028340	0.001237	0.067707	0.002563
16	0.025954	0.001238	0.063994	0.001960
17	0.024390	0.001217	0.065735	0.002479
18	0.022632	0.001108	0.067373	0.002818
19	0.021345	0.001059	0.068676	0.004088
20	0.020006	0.000984	0.066249	0.003637
21	0.018951	0.000957	0.061145	0.002794
22	0.017858	0.000909	0.065871	0.001829
23	0.017045	0.000874	0.068111	0.003389
24	0.016225	0.000802	0.065117	0.002602
25	0.015123	0.000801	0.064825	0.004219
26	0.014590	0.000695	0.066988	0.004510
27	0.014055	0.000773	0.063706	0.004390
28	0.013463	0.000749	0.065102	0.003806
29	0.012990	0.000707	0.069784	0.004774
30	0.012379	0.000634	0.065488	0.002930
31	0.011770	0.000688	0.066692	0.003860
32	0.011376	0.000579	0.068410	0.004704
33	0.011110	0.000627	0.064092	0.003423
34	0.010828	0.000590	0.065482	0.005664
35	0.010281	0.000576	0.070700	0.004654
36	0.009882	0.000591	0.066824	0.003957
37	0.009646	0.000484	0.067285	0.006001
38	0.009520	0.000476	0.065311	0.004340
39	0.009073	0.000565	0.069604	0.004400
40	0.008829	0.000464	0.071559	0.003640
41	0.008366	0.000435	0.065737	0.002700
42	0.007870	0.000331	0.066689	0.005454
43	0.007589	0.000406	0.065022	0.002306
44	0.007339	0.000318	0.062243	0.002440
45	0.007119	0.000332	0.058855	0.002890
46	0.006910	0.000367	0.064684	0.004572
47	0.007213	0.000395	0.062547	0.005711
48	0.006830	0.000379	0.071380	0.006341
49	0.006959	0.000402	0.062601	0.005942
50	0.006759	0.000403	0.057651	0.002235
51	0.006754	0.000349	0.058931	0.005104
52	0.006211	0.000369	0.072283	0.008650
53	0.006334	0.000384	0.070712	0.007567
54	0.005921	0.000273	0.063860	0.005764
55	0.005761	0.000494	0.065686	0.006616
56	0.005949	0.000413	0.061724	0.007175
57	0.005794	0.000523	0.067505	0.006876
58	0.005704	0.000417	0.060893	0.004392
59	0.005483	0.000346	0.071750	0.005602
60	0.005548	0.000388	0.067725	0.003863
61	0.005274	0.000336	0.077714	0.014022
62	0.005310	0.000405	0.068117	0.009394
63	0.005398	0.000477	0.065399	0.003948
64	0.005022	0.000259	0.073013	0.011405
65	0.005055	0.000399	0.070574	0.007386
66	0.005321	0.000307	0.060733	0.002698
67	0.004926	0.000344	0.062299	0.003728
68	0.004955	0.000475	0.059312	0.004677
69	0.004821	0.000218	0.053499	0.004569
70	0.004505	0.000479	0.056031	0.005231
71	0.004732	0.000350	0.082803	0.023144
72	0.004606	0.000325	0.068931	0.003605
73	0.004494	0.000220	0.059456	0.005713
74	0.004517	0.000296	0.068237	0.009014
75	0.004458	0.000366	0.057276	0.004027
76	0.003770	0.000248	0.049401	0.007636
77	0.003417	0.000000	0.050157	0.000000
78	0.003291	0.000000	0.041604	0.000000
79	0.002907	0.000000	0.049645	0.000000
80	0.002906	0.000000	0.055872	0.000000
81	0.003387	0.000000	0.048119	0.000000
82	0.003480	0.000000	0.052849	0.000000
83	0.003099	0.000000	0.047107	0.000000
84	0.002737	0.000000	0.041502	0.000000
85	0.002384	0.000000	0.048643	0.000000
86	0.003165	0.000000	0.037620	0.000000