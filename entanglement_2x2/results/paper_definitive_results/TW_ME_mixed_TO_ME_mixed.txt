#Tensor product hilbert space dimension: 4; Number of simulations: 100;
#Separable DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Maximally entangled DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_entangled.txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 40; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:10; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.99925%
#Sample standard deviation for averaged success rate: 0.00273758561277273%
#
#       Further info on simulation parameters
#
#       N=4;                                    
#       howManyTimes = 100;                     
#       architecture = [16,8,1];                
#       nepochs = 40;                           
#       fraction = 0.8;                         
#       actHL = 'relu';                         
#       lastAct = 'sigmoid';                    
#       loss_f = 'binary_crossentropy'          
#       batch_size = 40;                        
#       take_redundancy = False
#       output_file = cwd + '/results/paper_definitive_results/TW_ME_mixed_TO_ME_mixed.txt'
#       opt = 'rmsprop'                         
#       take_supplementary_tests = False        
#       tol = 0.5                               
#       study_val_loss = True                   
#       early_stopping = True                   
#       skipped_rows = (0, 0)
#       pre_shuffle = True                      
#       first_type = 'Separable'                
#       second_type= 'Maximally entangled'      
#       metric = 'val_loss'                     
#       epochs_to_wait = 10                     
#       min_delta = 1e-3      
#
#       loss, loss_std, ASR, ASRSTD, ASR2, ASRSTD2, \
#       val_loss, val_loss_std, reached_this_epoch, \
#       longest_training = binaryOutput_formatData_trainNN_averageLoss_averageTestResults_and_writeResults( 
#                       N, howManyTimes, 
#                       separable_filepath, first_type, 
#                       entangled_filepath, second_type, 
#                       architecture, nepochs, 
#                       fraction, 
#                       actHL, lastAct, loss_f, 
#                       batch_size, 
#                       take_redundancy=take_redundancy, 
#                       optimizer=opt, 
#                       perform_additional_tests=take_supplementary_tests, 
#                       first_test_filepath=additional_separable_filepaths, 
#                       second_test_filepath=additional_entangled_filepaths, 
#                       outFilePath = output_file, 
#                       tolerance=tol, 
#                       use_validation_data=study_val_loss, 
#                       trigger_early_stopping=early_stopping, 
#                       metric_to_monitor=metric, 
#                       epochs_patience=epochs_to_wait, 
#                       min_improvement=min_delta, 
#                       monitor_mode='min', 
#                       baseline=None, 
#                       recover_best_configuration=True, 
#                       first_label=0.0, 
#                       second_label=1.0, 
#                       shuffle=pre_shuffle, 
#                       rts=skipped_rows, 
#                       verb=0, 
#                       snitch_every=1)   
#
#                       reached_this_epoch=[100 100 100 100 100 100 100 100 100 100 100 100 100  96  50  18  10   6   4   3   2   2   1   1   1   1   1   1   1   1   1   1]               
#
#
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.575622	0.002902	0.400598	0.006072
2	0.216324	0.006200	0.084951	0.005109
3	0.038554	0.003307	0.014305	0.002131
4	0.007410	0.001560	0.003812	0.001216
5	0.002411	0.000985	0.001595	0.000799
6	0.001129	0.000662	0.000854	0.000543
7	0.000648	0.000453	0.000514	0.000370
8	0.000415	0.000311	0.000337	0.000254
9	0.000272	0.000211	0.000220	0.000172
10	0.000180	0.000143	0.000145	0.000117
11	0.000120	0.000097	0.000096	0.000079
12	0.000080	0.000066	0.000063	0.000053
13	0.000053	0.000044	0.000041	0.000036
14	0.000035	0.000031	0.000027	0.000025
15	0.000044	0.000040	0.000035	0.000033
16	0.000081	0.000074	0.000064	0.000060
17	0.000095	0.000088	0.000077	0.000071
18	0.000106	0.000095	0.000086	0.000078
19	0.000107	0.000092	0.000087	0.000075
20	0.000096	0.000078	0.000078	0.000064
21	0.000097	0.000069	0.000079	0.000056
22	0.000066	0.000047	0.000053	0.000038
23	0.000089	0.000000	0.000072	0.000000
24	0.000060	0.000000	0.000049	0.000000
25	0.000041	0.000000	0.000033	0.000000
26	0.000028	0.000000	0.000023	0.000000
27	0.000019	0.000000	0.000015	0.000000
28	0.000013	0.000000	0.000010	0.000000
29	0.000009	0.000000	0.000007	0.000000
30	0.000006	0.000000	0.000005	0.000000
31	0.000004	0.000000	0.000004	0.000000
32	0.000003	0.000000	0.000003	0.000000