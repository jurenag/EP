#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Mixed separable DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/received_from_DM/mixed_states/mixed_separable.txt; Mixed entangled states (Negativity in (.0, .1)) DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/generated/mixed_states/negativity_(0.0, 0.1).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 50; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.6275%
#Sample standard deviation for averaged success rate: 0.01950961301417124%
#Same average success rate for supplementary tests: [50.0165 50.911  62.7075 91.6025 99.889  99.871  99.55   99.643  99.888 99.9855]%
#Sample STD for averaged success rate in supplementary tests: [0.00696599 0.13372696 0.73014425 0.52976516 0.01938814 0.01016858 0.03061046 0.0240645  0.01594051 0.00455247]%
#
#       Further info on simulation parameters
# 
#       N=4;                               
#       howManyTimes = 10;                 
#       architecture = [16,8,1];           
#       nepochs = 50;                      
#       fraction = 0.8;                    
#       actHL = 'relu';                    
#       lastAct = 'sigmoid';               
#       loss_f = 'binary_crossentropy'     
#       batch_size = 40;                   
#       take_redundancy = False
#       output_file = cwd + '/results/paper_definitive_results/TW_M(.0,.1).txt' 
#       opt = 'rmsprop'                  
#       take_supplementary_tests = True  
#       tol = 0.5                        
#       study_val_loss = True            
#       early_stopping = True            
#       skipped_rows = (    0, 5,        
#                           0, 3, 0, 3, 0, 3, 0, 3, 0, 3,  
#                           #0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 
#                           0, 5, 0, 5, 0, 5, 0, 5, 0, 5)  
#       pre_shuffle = True                      
#       first_type = 'Mixed separable'          
#       second_type= 'Mixed entangled states (Negativity in (.0, .1))'   
#       metric = 'val_loss'                     
#       epochs_to_wait = 25                     
#       min_delta = 1e-3                        
#
#       loss, loss_std, ASR, ASRSTD, ASR2, ASRSTD2, \
#       val_loss, val_loss_std, reached_this_epoch, \
#       longest_training = binaryOutput_formatData_trainNN_averageLoss_averageTestResults_and_writeResults( 
#                       N, howManyTimes, 
#                       separable_filepath, first_type, 
#                       entangled_filepath, second_type, 
#                       architecture, nepochs, 
#                       fraction, 
#                       actHL, lastAct, loss_f, 
#                       batch_size, 
#                       take_redundancy=take_redundancy, 
#                       optimizer=opt, 
#                       perform_additional_tests=take_supplementary_tests, 
#                       first_test_filepath=additional_separable_filepaths, 
#                       second_test_filepath=additional_entangled_filepaths, 
#                       outFilePath = output_file, 
#                       tolerance=tol, 
#                       use_validation_data=study_val_loss, 
#                       trigger_early_stopping=early_stopping, 
#                       metric_to_monitor=metric, 
#                       epochs_patience=epochs_to_wait, 
#                       min_improvement=min_delta, 
#                       monitor_mode='min', 
#                       baseline=None, 
#                       recover_best_configuration=True, 
#                       first_label=0.0, 
#                       second_label=1.0, 
#                       shuffle=pre_shuffle, 
#                       rts=skipped_rows, 
#                       verb=0, 
#                       snitch_every=1)
#
#       reached_this_epoch, longest_training = [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10
#                                               10 10 10 10 10 10 10 10 10 10 10 10  9  9  9  8  6  6  4  3  3  3  3  3
#                                               2  2], 50
#
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.589230	0.007970	0.466567	0.017237
2	0.338216	0.022433	0.242094	0.023898
3	0.178044	0.022770	0.138598	0.020599
4	0.102131	0.018209	0.085268	0.015214
5	0.062447	0.013089	0.057548	0.010692
6	0.040876	0.009158	0.041720	0.007410
7	0.028476	0.006430	0.032467	0.005269
8	0.021008	0.004533	0.026969	0.003831
9	0.016357	0.003263	0.023303	0.002948
10	0.013041	0.002363	0.021111	0.002238
11	0.010811	0.001756	0.019406	0.001680
12	0.009169	0.001358	0.018057	0.001486
13	0.008021	0.001120	0.017062	0.001474
14	0.007153	0.000942	0.017602	0.001119
15	0.006431	0.000800	0.016247	0.001268
16	0.005891	0.000717	0.015820	0.001090
17	0.005381	0.000638	0.014963	0.001186
18	0.005064	0.000602	0.016979	0.001424
19	0.004765	0.000564	0.015460	0.001253
20	0.004418	0.000520	0.015787	0.000995
21	0.004186	0.000505	0.017057	0.001005
22	0.003972	0.000489	0.016053	0.000840
23	0.003823	0.000474	0.016000	0.001161
24	0.003612	0.000435	0.016690	0.000990
25	0.003465	0.000420	0.016152	0.001273
26	0.003242	0.000415	0.016038	0.001169
27	0.003113	0.000403	0.016237	0.001538
28	0.002997	0.000375	0.015702	0.001407
29	0.002880	0.000389	0.016751	0.001533
30	0.002758	0.000378	0.016862	0.001070
31	0.002674	0.000355	0.016865	0.001518
32	0.002559	0.000346	0.017094	0.001111
33	0.002455	0.000345	0.018315	0.001932
34	0.002369	0.000320	0.018014	0.001576
35	0.002303	0.000319	0.018261	0.001230
36	0.002243	0.000319	0.017062	0.001174
37	0.002203	0.000346	0.017446	0.001375
38	0.002082	0.000328	0.017310	0.001205
39	0.002003	0.000308	0.017471	0.001145
40	0.002068	0.000336	0.018099	0.001547
41	0.002435	0.000285	0.019109	0.001860
42	0.002328	0.000289	0.018658	0.001792
43	0.002366	0.000116	0.019561	0.002227
44	0.002416	0.000203	0.020156	0.002573
45	0.002075	0.000194	0.023198	0.002868
46	0.002311	0.000112	0.023567	0.003555
47	0.001999	0.000116	0.024557	0.004617
48	0.002168	0.000079	0.020426	0.002705
49	0.002295	0.000087	0.022341	0.002641
50	0.002103	0.000024	0.023889	0.004903