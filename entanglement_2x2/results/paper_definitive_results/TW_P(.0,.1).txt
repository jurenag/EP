#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Pure separable DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Pure minimally entangled states (Negativity in (.0, .1)) DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/generated/pure_states/juliolike/negativity_(0.0, 0.1).txt;
#Architecture of the MLP: [128, 64, 32, 16, 1]; Number of epochs: 120; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 94.2825%
#Sample standard deviation for averaged success rate: 0.16613435827687242%
#Same average success rate for supplementary tests: [97.1325 99.1305 99.146  99.1465 99.1465 98.314  98.3805 98.3805 98.381 98.381 ]%
#Sample STD for averaged success rate in supplementary tests: [0.17740103 0.10642474 0.10449833 0.10436966 0.10436966 0.14447283 0.14510074 0.14510074 0.14515475 0.14515475]%
#
#       Further info on simulation parameters
#
#       N=4;                                  
#       howManyTimes = 10;                    
#       architecture = [128,64,32,16,1];      
#       nepochs = 120;                        
#       fraction = 0.8;                       
#       actHL = 'relu';                       
#       lastAct = 'sigmoid';                  
#       loss_f = 'binary_crossentropy'        
#       batch_size = 40;                      
#       take_redundancy = False
#       output_file = cwd + '/results/paper_definitive_results/TW_P(.0,.1).txt'
#       opt = 'rmsprop'                        
#       take_supplementary_tests = True        
#       tol = 0.5                              
#       study_val_loss = True                  
#       early_stopping = True                  
#       skipped_rows = (    0, 3,                           
#                           0, 3, 0, 3, 0, 3, 0, 3, 0, 3,   
#                           #0, 3, 0, 3, 0, 3, 0, 3, 0, 3,  
#                           0, 5, 0, 5, 0, 5, 0, 5, 0, 5)   
#       pre_shuffle = True                      
#       first_type = 'Pure separable'           
#       second_type= 'Pure minimally entangled states (Negativity in (.0, .1))'   
#       metric = 'val_loss'                 
#       epochs_to_wait = 25                 
#       min_delta = 1e-3                    
#
#       loss, loss_std, ASR, ASRSTD, ASR2, ASRSTD2, \
#       val_loss, val_loss_std, reached_this_epoch, \
#       longest_training = binaryOutput_formatData_trainNN_averageLoss_averageTestResults_and_writeResults( 
#                       N, howManyTimes, 
#                       separable_filepath, first_type, 
#                       entangled_filepath, second_type, 
#                       architecture, nepochs, 
#                       fraction, 
#                       actHL, lastAct, loss_f, 
#                       batch_size, 
#                       take_redundancy=take_redundancy, 
#                       optimizer=opt, 
#                       perform_additional_tests=take_supplementary_tests, 
#                       first_test_filepath=additional_separable_filepaths, 
#                       second_test_filepath=additional_entangled_filepaths, 
#                       outFilePath = output_file, 
#                       tolerance=tol, 
#                       use_validation_data=study_val_loss, 
#                       trigger_early_stopping=early_stopping, 
#                       metric_to_monitor=metric, 
#                       epochs_patience=epochs_to_wait, 
#                       min_improvement=min_delta, 
#                       monitor_mode='min', 
#                       baseline=None, 
#                       recover_best_configuration=True, 
#                       first_label=0.0, 
#                       second_label=1.0, 
#                       shuffle=pre_shuffle, 
#                       rts=skipped_rows, 
#                       verb=0, 
#                       snitch_every=1)
#
#       reached_this_epoch, longest_training = [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10
#                                               10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10
#                                               10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10
#                                               9  9  9  9  9  9  9  9  9  9  8  7  7  7  7  7  7  6  6  5  5  5  5  5
#                                               5  5  5  5  4  3  3  2  2  2  2  2  2  2  2  2  1  1  1  1  1  1  1  1], 120
#
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.692420	0.000101	0.691470	0.000239
2	0.688711	0.000254	0.688634	0.000394
3	0.684943	0.000300	0.686755	0.000721
4	0.681473	0.000295	0.683940	0.000712
5	0.678775	0.000318	0.683722	0.000816
6	0.676275	0.000329	0.680747	0.000806
7	0.673468	0.000466	0.679501	0.000539
8	0.670430	0.000686	0.679802	0.001207
9	0.666353	0.001093	0.676292	0.001120
10	0.660809	0.001706	0.673895	0.002216
11	0.653032	0.002812	0.664632	0.003276
12	0.641722	0.004314	0.654321	0.005590
13	0.625995	0.006378	0.637284	0.007050
14	0.604610	0.008919	0.617311	0.010513
15	0.576272	0.012084	0.591938	0.013079
16	0.540861	0.015414	0.556404	0.015288
17	0.503731	0.018522	0.520790	0.018579
18	0.466578	0.021422	0.477461	0.022720
19	0.431389	0.023380	0.455270	0.023046
20	0.400054	0.024342	0.429284	0.024723
21	0.371900	0.025005	0.422275	0.021581
22	0.346550	0.024668	0.387151	0.021763
23	0.324600	0.023376	0.359679	0.022423
24	0.303386	0.022224	0.346504	0.022287
25	0.285447	0.020449	0.335382	0.018843
26	0.268891	0.018322	0.323356	0.014059
27	0.254994	0.017142	0.305694	0.015124
28	0.241232	0.015109	0.290738	0.015053
29	0.230000	0.013754	0.287189	0.016076
30	0.218939	0.012323	0.284422	0.013708
31	0.209242	0.011664	0.271592	0.010920
32	0.199861	0.010727	0.266411	0.011327
33	0.193337	0.009900	0.258715	0.008543
34	0.186351	0.009569	0.241291	0.008441
35	0.178885	0.008804	0.261164	0.008556
36	0.172794	0.008352	0.256049	0.010418
37	0.167943	0.007808	0.239799	0.010543
38	0.162314	0.007595	0.251241	0.010796
39	0.157629	0.007626	0.243091	0.010803
40	0.152704	0.006914	0.235254	0.009397
41	0.148284	0.006739	0.222072	0.008434
42	0.144215	0.006608	0.230090	0.007904
43	0.142042	0.006334	0.221817	0.007769
44	0.137756	0.006307	0.218006	0.006090
45	0.133428	0.005782	0.219407	0.004651
46	0.131377	0.005531	0.217069	0.006022
47	0.127860	0.005717	0.217152	0.006639
48	0.125027	0.004941	0.222993	0.006176
49	0.121548	0.005252	0.215332	0.005484
50	0.118863	0.004820	0.219111	0.007387
51	0.115478	0.004657	0.209452	0.008075
52	0.113161	0.005106	0.231872	0.012762
53	0.110929	0.004607	0.220421	0.008075
54	0.109017	0.004328	0.213248	0.006794
55	0.106869	0.004194	0.220259	0.008642
56	0.104628	0.004100	0.204638	0.005242
57	0.102558	0.003861	0.212835	0.008276
58	0.100386	0.004062	0.211273	0.006192
59	0.098799	0.003941	0.212235	0.006635
60	0.096991	0.003770	0.205000	0.005656
61	0.095945	0.003560	0.220981	0.014282
62	0.093610	0.003390	0.212053	0.007238
63	0.093251	0.003544	0.212837	0.005784
64	0.090911	0.003366	0.199989	0.007213
65	0.088760	0.003411	0.208402	0.007649
66	0.087235	0.003309	0.209215	0.009186
67	0.086112	0.003499	0.208100	0.006030
68	0.085534	0.003272	0.206653	0.005349
69	0.083663	0.003340	0.214081	0.006835
70	0.082528	0.003283	0.205385	0.006660
71	0.081081	0.003152	0.201958	0.003555
72	0.080629	0.002920	0.201471	0.005350
73	0.078630	0.003680	0.202930	0.003948
74	0.076854	0.003184	0.218554	0.010210
75	0.076256	0.003016	0.205891	0.008651
76	0.075629	0.002890	0.198327	0.008049
77	0.073791	0.003031	0.209261	0.006058
78	0.072684	0.003190	0.207008	0.006976
79	0.071683	0.003143	0.203872	0.010627
80	0.072037	0.003320	0.202324	0.006586
81	0.071210	0.002909	0.196551	0.004839
82	0.070588	0.003240	0.205775	0.005628
83	0.070839	0.003022	0.208232	0.005184
84	0.070028	0.003430	0.206734	0.009094
85	0.070032	0.002972	0.201891	0.008031
86	0.069507	0.003679	0.214428	0.006805
87	0.067128	0.003669	0.200676	0.006652
88	0.066922	0.003159	0.211159	0.005002
89	0.067587	0.003446	0.206477	0.005700
90	0.067377	0.002676	0.212157	0.006426
91	0.066352	0.003494	0.217171	0.011456
92	0.067966	0.003765	0.209470	0.009057
93	0.067798	0.003832	0.206482	0.011321
94	0.067372	0.003570	0.219858	0.013585
95	0.067053	0.003318	0.228598	0.004816
96	0.065961	0.003566	0.214391	0.011314
97	0.064867	0.003588	0.198236	0.010230
98	0.063510	0.003221	0.212182	0.012257
99	0.062184	0.003826	0.221930	0.008417
100	0.062287	0.003192	0.210767	0.003729
101	0.061345	0.004011	0.217235	0.009093
102	0.065175	0.002859	0.208419	0.003484
103	0.063551	0.001789	0.237525	0.009885
104	0.062308	0.005839	0.232565	0.008689
105	0.063742	0.003295	0.221554	0.004966
106	0.062425	0.004370	0.217517	0.005449
107	0.063136	0.003509	0.221538	0.011741
108	0.062349	0.004678	0.211623	0.001477
109	0.061514	0.004271	0.218492	0.003372
110	0.061916	0.004474	0.224681	0.002401
111	0.060932	0.003487	0.231456	0.012894
112	0.062207	0.003403	0.211526	0.002669
113	0.057291	0.000000	0.204382	0.000000
114	0.053137	0.000000	0.199191	0.000000
115	0.053642	0.000000	0.216195	0.000000
116	0.053188	0.000000	0.203158	0.000000
117	0.051149	0.000000	0.178195	0.000000
118	0.051762	0.000000	0.224692	0.000000
119	0.051981	0.000000	0.206250	0.000000
120	0.051821	0.000000	0.230683	0.000000