#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Pure separable DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Pure entangled states (Negativity in (.3, .4)) DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/generated/pure_states/juliolike/negativity_(0.3, 0.4).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 70; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.94000000000001%
#Sample standard deviation for averaged success rate: 0.022135943611591356%
#Same average success rate for supplementary tests: [50.0985 55.5625 87.028  99.985  99.9975 95.078  98.778  99.9095 99.9935 99.9935]%
#Sample STD for averaged success rate in supplementary tests: [1.88686248e-02 5.80584296e-01 8.36911644e-01 5.83095190e-03 1.06066011e-03 1.20499071e+00 2.54766756e-01 1.22362167e-02 2.65047164e-03 2.65047164e-03]%
#
#       Further info on simulation parameters
# 
#       N=4;                                
#       howManyTimes = 10;                  
#       architecture = [16,8,1];            
#       nepochs = 70;                       
#       fraction = 0.8;                     
#       actHL = 'relu';                     
#       lastAct = 'sigmoid';                
#       loss_f = 'binary_crossentropy'      
#       batch_size = 40;                    
#       take_redundancy = False
#       output_file = cwd + '/results/paper_definitive_results/TW_P(.3,.4).txt' 
#       opt = 'rmsprop'                   
#       take_supplementary_tests = True   
#       tol = 0.5                         
#       study_val_loss = True             
#       early_stopping = True             
#       skipped_rows = (    0, 3,         
#                           0, 3, 0, 3, 0, 3, 0, 3, 0, 3,  
#                           #0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 
#                           0, 5, 0, 5, 0, 5, 0, 5, 0, 5)  
#       pre_shuffle = True                  
#       first_type = 'Pure separable'       
#       second_type= 'Pure entangled states (Negativity in (.3, .4))'    
#       metric = 'val_loss'                  
#       epochs_to_wait = 25                  
#       min_delta = 1e-3                     
#
#       loss, loss_std, ASR, ASRSTD, ASR2, ASRSTD2, \
#       val_loss, val_loss_std, reached_this_epoch, \
#       longest_training = binaryOutput_formatData_trainNN_averageLoss_averageTestResults_and_writeResults( 
#                       N, howManyTimes, 
#                       separable_filepath, first_type, 
#                       entangled_filepath, second_type, 
#                       architecture, nepochs, 
#                       fraction, 
#                       actHL, lastAct, loss_f, 
#                       batch_size, 
#                       take_redundancy=take_redundancy, 
#                       optimizer=opt, 
#                       perform_additional_tests=take_supplementary_tests, 
#                       first_test_filepath=additional_separable_filepaths, 
#                       second_test_filepath=additional_entangled_filepaths, 
#                       outFilePath = output_file, 
#                       tolerance=tol, 
#                       use_validation_data=study_val_loss, 
#                       trigger_early_stopping=early_stopping, 
#                       metric_to_monitor=metric, 
#                       epochs_patience=epochs_to_wait, 
#                       min_improvement=min_delta, 
#                       monitor_mode='min', 
#                       baseline=None, 
#                       recover_best_configuration=True, 
#                       first_label=0.0, 
#                       second_label=1.0, 
#                       shuffle=pre_shuffle, 
#                       rts=skipped_rows, 
#                       verb=0, 
#                       snitch_every=1)
#
#       reached_this_epoch, longest_training = [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10
#                                               10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10
#                                               10 10 10 10 10 10 10 10 10 10 10 10  9  8  8  8  8  8  8  8  8  8], 70
#
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.662188	0.002159	0.620404	0.004622
2	0.558841	0.007942	0.497754	0.011008
3	0.443045	0.011961	0.403208	0.012330
4	0.363030	0.012735	0.339353	0.013211
5	0.302426	0.014066	0.284660	0.014656
6	0.246961	0.015517	0.231152	0.016010
7	0.195413	0.015952	0.181060	0.016276
8	0.151187	0.015111	0.139341	0.014877
9	0.115126	0.013105	0.106035	0.012063
10	0.086987	0.010615	0.080099	0.009843
11	0.065865	0.008219	0.061799	0.007166
12	0.050594	0.006366	0.047232	0.005581
13	0.039711	0.005010	0.038148	0.004417
14	0.031883	0.004013	0.031909	0.003772
15	0.026116	0.003341	0.026172	0.003161
16	0.021684	0.002755	0.022246	0.002796
17	0.018349	0.002395	0.018949	0.002330
18	0.015759	0.002052	0.016688	0.002136
19	0.013583	0.001817	0.015080	0.001950
20	0.011899	0.001609	0.013247	0.001791
21	0.010395	0.001428	0.011666	0.001506
22	0.009247	0.001310	0.010655	0.001408
23	0.008196	0.001159	0.009722	0.001301
24	0.007278	0.001067	0.009336	0.001326
25	0.006644	0.000990	0.007746	0.001138
26	0.005889	0.000913	0.007487	0.001069
27	0.005335	0.000826	0.007075	0.001149
28	0.004828	0.000767	0.006590	0.001193
29	0.004462	0.000739	0.006247	0.001118
30	0.004085	0.000682	0.005863	0.000996
31	0.003697	0.000634	0.005494	0.000824
32	0.003405	0.000601	0.004975	0.000889
33	0.003201	0.000549	0.004977	0.001030
34	0.002919	0.000521	0.004459	0.000812
35	0.002680	0.000477	0.004250	0.000819
36	0.002492	0.000472	0.004307	0.000973
37	0.002363	0.000444	0.003663	0.000709
38	0.002153	0.000414	0.003657	0.000753
39	0.002097	0.000416	0.003610	0.000712
40	0.001930	0.000378	0.003396	0.000645
41	0.001843	0.000375	0.003372	0.000758
42	0.001721	0.000350	0.003040	0.000639
43	0.001613	0.000329	0.003160	0.000772
44	0.001536	0.000319	0.003029	0.000671
45	0.001441	0.000299	0.002907	0.000605
46	0.001345	0.000283	0.002710	0.000545
47	0.001303	0.000279	0.002820	0.000721
48	0.001180	0.000251	0.002684	0.000697
49	0.001129	0.000240	0.002672	0.000654
50	0.001063	0.000227	0.002797	0.000765
51	0.001030	0.000215	0.002583	0.000833
52	0.000980	0.000217	0.002314	0.000618
53	0.000914	0.000193	0.002446	0.000629
54	0.000874	0.000193	0.002251	0.000524
55	0.000804	0.000178	0.002294	0.000628
56	0.000776	0.000177	0.002821	0.000762
57	0.000732	0.000170	0.002644	0.000663
58	0.000729	0.000175	0.002115	0.000609
59	0.000669	0.000155	0.001974	0.000635
60	0.000648	0.000157	0.002116	0.000615
61	0.000680	0.000142	0.002133	0.000667
62	0.000741	0.000148	0.002733	0.000686
63	0.000681	0.000135	0.002427	0.000829
64	0.000645	0.000135	0.002123	0.000619
65	0.000645	0.000138	0.002249	0.000726
66	0.000584	0.000130	0.002070	0.000559
67	0.000570	0.000127	0.002472	0.000787
68	0.000546	0.000123	0.002430	0.000842
69	0.000571	0.000136	0.002108	0.000706
70	0.000523	0.000128	0.002452	0.000974