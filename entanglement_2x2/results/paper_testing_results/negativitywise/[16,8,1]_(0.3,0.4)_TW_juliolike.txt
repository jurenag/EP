#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled w/ [0.3,0.4] neg. juliolike DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/generated/pure_states/juliolike/negativity_(0.3, 0.4).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 30; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.845%
#Sample standard deviation for averaged success rate: 0.42142318398495254%
#Same average success rate for supplementary tests: [0.50075, 0.5476449999999999, 0.85911, 0.99944, 0.9999299999999999, 0.501585, 0.5589599999999999, 0.865275, 0.9987, 0.9999299999999999, 0.907185, 0.9758049999999999, 0.99823, 0.9999099999999999, 0.9999099999999999]%
#Sample STD for averaged success rate in supplementary tests: [0.157861786857998, 0.14584802157554283, 0.07226963947606212, 0.0024836746968964823, 0.0008363671442824002, 0.15762486725609004, 0.14293799998600798, 0.06959143221331202, 0.003825049019292064, 0.0008363671442824002, 0.04773245832240371, 0.025418697793160375, 0.006089885877420991, 0.0009482562944882443, 0.0009482562944882443]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.660980	0.002375	0.615827	0.005032
2	0.555986	0.007407	0.493139	0.008844
3	0.441107	0.008741	0.394050	0.007908
4	0.356499	0.007361	0.326313	0.007629
5	0.295071	0.008338	0.271317	0.009005
6	0.241252	0.010391	0.220918	0.011066
7	0.191652	0.011883	0.173611	0.011598
8	0.147651	0.011804	0.132597	0.011189
9	0.111268	0.010176	0.099352	0.009228
10	0.082719	0.008163	0.073820	0.007190
11	0.061272	0.006253	0.055179	0.005501
12	0.045761	0.004689	0.042262	0.004168
13	0.034853	0.003606	0.032606	0.003313
14	0.027213	0.002935	0.026116	0.002782
15	0.021738	0.002449	0.021579	0.002405
16	0.017788	0.002122	0.018327	0.002249
17	0.014739	0.001879	0.015501	0.001955
18	0.012368	0.001678	0.014631	0.002221
19	0.010553	0.001505	0.012255	0.001891
20	0.009112	0.001373	0.010510	0.001588
21	0.008001	0.001256	0.009607	0.001459
22	0.007075	0.001148	0.008833	0.001297
23	0.006244	0.001051	0.007959	0.001294
24	0.005565	0.000965	0.007352	0.001204
25	0.004965	0.000842	0.006844	0.001038
26	0.004418	0.000786	0.006560	0.001025
27	0.003996	0.000715	0.006076	0.000885
28	0.003623	0.000628	0.006065	0.000939
29	0.003276	0.000577	0.005586	0.000939
30	0.002984	0.000527	0.005533	0.000773
