#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled w/ [0.2,0.3] neg. antoniolike DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/generated/pure_states/antoniolike_1/negativity_(0.2, 0.3).txt;
#Architecture of the MLP: [32, 16, 8, 1]; Number of epochs: 100; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.905%
#Sample standard deviation for averaged success rate: 0.3147975539932686%
#Same average success rate for supplementary tests: [0.516005, 0.80747, 0.998445, 0.999885, 0.999885, 0.5249149999999999, 0.8265049999999999, 0.99975, 0.999885, 0.999885, 0.9637400000000002, 0.997705, 0.9993799999999999, 0.9993799999999999, 0.9993799999999999]%
#Sample STD for averaged success rate in supplementary tests: [0.153708926212826, 0.09191583057341106, 0.004148276449321577, 0.001071763733295222, 0.001071763733295222, 0.1512916860818862, 0.08581723892959979, 0.0016105123408411768, 0.001071763733284863, 0.001071763733284863, 0.030290613067417014, 0.0059264911625697275, 0.002619457959199856, 0.002619457959199856, 0.002619457959199856]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.655905	0.002046	0.602597	0.004343
2	0.548871	0.005280	0.503350	0.006680
3	0.465641	0.010014	0.425273	0.013526
4	0.376540	0.017453	0.325349	0.020248
5	0.265639	0.020343	0.211534	0.019622
6	0.165975	0.016050	0.129220	0.013150
7	0.103803	0.010419	0.085357	0.008804
8	0.069678	0.006826	0.061356	0.005323
9	0.050221	0.005028	0.046858	0.004172
10	0.038456	0.003899	0.038879	0.004499
11	0.030688	0.003174	0.031099	0.003420
12	0.024859	0.002746	0.026301	0.002643
13	0.020916	0.002289	0.028710	0.003231
14	0.017743	0.001996	0.021471	0.002504
15	0.015373	0.001852	0.020016	0.002213
16	0.013199	0.001642	0.018443	0.002981
17	0.011928	0.001475	0.015066	0.002099
18	0.010845	0.001385	0.015421	0.001678
19	0.009598	0.001232	0.015274	0.002342
20	0.008888	0.001279	0.013483	0.002366
21	0.007681	0.001053	0.012870	0.002198
22	0.007080	0.001062	0.011388	0.002340
23	0.006595	0.000890	0.010761	0.001364
24	0.005901	0.000787	0.011619	0.001552
25	0.005539	0.000897	0.011182	0.001535
26	0.004932	0.000799	0.009910	0.001721
27	0.004696	0.000736	0.009462	0.001293
28	0.004403	0.000691	0.010164	0.001151
29	0.003884	0.000660	0.008761	0.001131
30	0.003899	0.000681	0.007588	0.001075
31	0.003593	0.000581	0.008410	0.000842
32	0.003382	0.000608	0.011475	0.003995
33	0.003237	0.000612	0.006783	0.000879
34	0.002874	0.000419	0.008118	0.001554
35	0.002741	0.000532	0.006223	0.000849
36	0.002703	0.000505	0.007512	0.001351
37	0.002421	0.000470	0.010168	0.002337
38	0.002517	0.000400	0.008629	0.002154
39	0.002228	0.000417	0.008677	0.001674
40	0.002157	0.000361	0.007806	0.000912
41	0.001995	0.000404	0.007244	0.000675
42	0.002116	0.000448	0.012422	0.006647
43	0.001943	0.000401	0.006696	0.000977
44	0.001846	0.000389	0.008574	0.001879
45	0.001625	0.000357	0.008561	0.002993
46	0.001651	0.000346	0.006256	0.001086
47	0.001722	0.000301	0.006957	0.000825
48	0.001624	0.000374	0.007695	0.001819
49	0.001651	0.000309	0.008313	0.002256
50	0.001388	0.000254	0.007239	0.001226
51	0.001235	0.000292	0.006511	0.001072
52	0.001339	0.000319	0.006292	0.000884
53	0.001334	0.000259	0.009487	0.002089
54	0.001226	0.000276	0.007162	0.001507
55	0.001291	0.000349	0.005336	0.001165
56	0.001235	0.000245	0.006632	0.001637
57	0.001047	0.000256	0.007996	0.001435
58	0.001211	0.000221	0.006314	0.001783
59	0.001132	0.000295	0.007837	0.003233
60	0.001051	0.000267	0.006634	0.001866
61	0.000945	0.000223	0.006813	0.001072
62	0.001000	0.000273	0.006264	0.001376
63	0.001018	0.000344	0.005521	0.000933
64	0.000933	0.000241	0.005618	0.000892
65	0.000770	0.000239	0.006030	0.001409
66	0.000850	0.000179	0.007882	0.002560
67	0.000965	0.000288	0.010999	0.004690
68	0.000814	0.000177	0.007997	0.002535
69	0.001082	0.000356	0.006822	0.001428
70	0.000934	0.000352	0.007455	0.001139
71	0.001041	0.000431	0.008699	0.002118
72	0.000993	0.000282	0.024108	0.012430
73	0.001294	0.000374	0.011366	0.004439
74	0.001072	0.000412	0.006369	0.001124
75	0.001226	0.000508	0.014568	0.004885
76	0.001009	0.000285	0.005524	0.001369
77	0.001130	0.000313	0.010445	0.005105
78	0.000865	0.000252	0.007485	0.001062
79	0.001120	0.000364	0.006130	0.001614
80	0.000639	0.000167	0.007819	0.002682
81	0.001039	0.000300	0.007197	0.001985
82	0.001358	0.000394	0.007041	0.002566
83	0.000733	0.000298	0.004419	0.001162
84	0.000455	0.000116	0.011409	0.001683
85	0.000525	0.000156	0.014737	0.008793
86	0.001583	0.000000	0.006236	0.000000
87	0.000810	0.000000	0.010396	0.000000
88	0.001747	0.000000	0.003379	0.000000
89	0.001807	0.000000	0.005619	0.000000
90	0.001594	0.000000	0.012518	0.000000
