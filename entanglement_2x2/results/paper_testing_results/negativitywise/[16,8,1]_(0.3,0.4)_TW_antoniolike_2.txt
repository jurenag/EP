#Tensor product hilbert space dimension: 4; Number of simulations: 10;
#Separable DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/received_from_DM/pure_states/separable.txt; Entangled w/ [0.3,0.4] neg. antoniolike DMs were read from: /home/julio/Documents/entanglement_paper/updated_TFM_code/entanglement_2x2/input_data/generated/pure_states/antoniolike_2/negativity_(0.3, 0.4).txt;
#Architecture of the MLP: [16, 8, 1]; Number of epochs: 30; Fraction of DMs used for training: 0.8;
#Activation function in the hidden layers: relu; Activation function in the output layer: sigmoid; Loss function: binary_crossentropy;
#Optimizer: rmsprop; Batch size: 40; Test tolerance: 0.5;
#tf.Keras.callbacks.EarlyStopping was used with: metric:val_loss; Epochs patience:25; Minimum improvement:0.001;
#Sucess rate averaged over every simulation and over every sample in the test set: 99.85000000000001%
#Sample standard deviation for averaged success rate: 0.3908324449171586%
#Same average success rate for supplementary tests: [0.50125, 0.552685, 0.85945, 0.9985599999999999, 0.9998799999999999, 0.50244, 0.562515, 0.8652549999999999, 0.99936, 0.9998799999999999, 0.969325, 0.9907099999999999, 0.9987550000000001, 0.9997349999999998, 0.99974]%
#Sample STD for averaged success rate in supplementary tests: [0.15769224378516528, 0.14455960389230457, 0.07125426127608088, 0.0040734064368810295, 0.0011395437683644084, 0.15735566287871564, 0.14182643433965336, 0.06865878310529566, 0.0025805115771836513, 0.0011395437683644084, 0.03067172048483771, 0.01769094655466558, 0.005214882309312423, 0.0016257236850195718, 0.0016103539983472067]%
#Epoch	Loss	Loss sample STD	Val. Loss	V.L. sample STD
1	0.664497	0.002120	0.624840	0.003537
2	0.569323	0.004563	0.510187	0.004931
3	0.456939	0.006795	0.410764	0.007328
4	0.375803	0.009625	0.345967	0.010469
5	0.315373	0.012760	0.289817	0.013195
6	0.260012	0.015286	0.236986	0.015752
7	0.208857	0.016706	0.188831	0.016251
8	0.163898	0.016613	0.147232	0.015520
9	0.125924	0.014844	0.114008	0.013344
10	0.095760	0.012344	0.087455	0.010989
11	0.073131	0.010008	0.067989	0.008721
12	0.056500	0.007913	0.053770	0.007103
13	0.044298	0.006171	0.044315	0.005784
14	0.035237	0.004852	0.035717	0.004370
15	0.028553	0.003824	0.030109	0.003779
16	0.023516	0.003072	0.025019	0.003079
17	0.019649	0.002482	0.021278	0.002612
18	0.016630	0.002046	0.018756	0.002332
19	0.014244	0.001757	0.016696	0.001934
20	0.012326	0.001503	0.015163	0.001953
21	0.010851	0.001336	0.013181	0.001541
22	0.009547	0.001173	0.012626	0.001556
23	0.008514	0.001059	0.011045	0.001288
24	0.007520	0.000946	0.010446	0.001332
25	0.006763	0.000865	0.009971	0.001415
26	0.006111	0.000794	0.009170	0.000965
27	0.005543	0.000726	0.008274	0.001053
28	0.004999	0.000661	0.007459	0.000775
29	0.004500	0.000601	0.007265	0.000801
30	0.004159	0.000564	0.006466	0.000742
